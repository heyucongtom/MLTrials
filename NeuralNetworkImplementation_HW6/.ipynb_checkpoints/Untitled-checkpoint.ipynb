{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Code.\n",
    "\n",
    "The part below is the basic network part.\n",
    "\n",
    "Q2: Train this multi-layer neural network on full training data using stochastic gradient descent. Predict the labels to the test data and submit your results to Kaggle. Please also report the following:\n",
    "\n",
    "• Parameters that you tuned including learning rate, when you stopped train-ing, how you initialized the weights\n",
    "\n",
    "Ans: I use the learning rate of 0.01. I include a save and load in the network to output the weight, and I stop training based on each response from the training set. For each epoch I monitor its accuracy. I initialize the weights with a Gaussian with mean 0 and theta 0.1\n",
    "\n",
    "=========================\n",
    "\n",
    "• Training accuracy and validation accuracy\n",
    "\n",
    "Ans: Training accuracy in the 50000 sample is about 96%, same for the test set.\n",
    "\n",
    "=========================\n",
    "\n",
    "• Running-time (Total training time)\n",
    "\n",
    "Ans: Total running time is about approximate 10 min to run through the code, for the basic network.\n",
    "\n",
    "=========================\n",
    "\n",
    "• Plots of total training error and classification accuracy on training set vs. iteration. If you find that evaluating error takes a long time, you may compute the error or accuracy every 1000 or so iterations.\n",
    "\n",
    "Ans: See the plot below.\n",
    "\n",
    "=========================\n",
    "\n",
    "• Comment on the differences in using the two loss functions. Which performs better?\n",
    "\n",
    "Ans: The sigmoid function give a more continuous performance. With the tanh it seems to be more jumpy on the edge values. Don't know why yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111357ac8>]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRURK3P8Dy6RSQ24T8PEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d1cf588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't run this code at first pass.\n",
    "# This is just for the answer purpose.\n",
    "# The accuracy is noted below, which reach 96.1% on Kaggle.\n",
    "# plt.plot(list(map(lambda k:1-k, error_lst2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testPrint(content, arg):\n",
    "    print(content+\"{0}\".format(arg))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Apply sigmoid activation function\n",
    "    # Support\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "class QuadraticCost:\n",
    "    \"\"\" Cost functions for quadratic cost. \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(activations, targets):\n",
    "        \"\"\" Evaluate quadratic cost. \"\"\"\n",
    "        return 0.5 * (activations - targets)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def fn_deriv(activations, targets):\n",
    "        \"\"\" Evaluate derivative of quadratic cost. \"\"\"\n",
    "        return activations - targets\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(inputs, activations, targets):\n",
    "        \"\"\" Compute the delta error at the output layer for the quadratic cost. \"\"\"\n",
    "        return (activations - targets)*sigmoid_deriv(inputs)\n",
    "    \n",
    "class CrossEntropyCost:\n",
    "    \"\"\" Cost functions for cross entropy cost. \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(activations, targets):\n",
    "        \"\"\" Evaluate cross entropy cost. \"\"\"\n",
    "        # The np.nan_to_num function ensures that np.log(0) evaluates to 0 instead of nan.\n",
    "        return -np.nan_to_num(targets*np.log(activations) + \\\n",
    "                                                  (1-targets)*np.log(1 - activations))  \n",
    "    @staticmethod\n",
    "    def fn_deriv(activations, targets):\n",
    "        \"\"\" Evaluate the derivative of the cross entropy cost. \"\"\"\n",
    "        return -np.nan_to_num(targets/activations - (1-targets)/(1-activations))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(inputs, activations, targets):\n",
    "        \"\"\" Compute the delta error at the output layer for the cross entropy cost. \"\"\"\n",
    "        return (activations-targets)\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Synapse: take a value and multiply by its weight.\n",
    "    Neuron: z = x1 + x2 + x3...\n",
    "            a = 1/(1 + e^-z)\n",
    "    \"\"\"\n",
    "    def __init__(self, cost=CrossEntropyCost):\n",
    "        self.inputLayerSize = 784\n",
    "        self.outputLayerSize = 10\n",
    "        self.hiddenLayerSize = 200\n",
    "        \n",
    "        self.num_of_layers = 1\n",
    "        \n",
    "        self.cost = cost\n",
    "\n",
    "        self.W1 = 0.1 * np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = 0.1 * np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        self.bias2 = 0.1 * np.random.randn(self.hiddenLayerSize)\n",
    "        self.bias3 = 0.1 * np.random.randn(self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Propagate multiple input through matrix\n",
    "        Do matrix multiplication by input layer and weight on synapse.\n",
    "\n",
    "        X * W(1) = Z(2)\n",
    "        Z is row(example)(784) * col(hidden)(200)\n",
    "\n",
    "        a(2) = s(Z(2))\n",
    "        a is 200 x 200\n",
    "\n",
    "        z(3) = a(2)W(2)\n",
    "        z(3) is 200 * 10\n",
    "\n",
    "        y = f(z(3))\n",
    "        \"\"\"\n",
    "        self.z2 = np.dot(X, self.W1) + self.bias2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2) + self.bias3\n",
    "        yHat = sigmoid(self.z3)\n",
    "        return yHat\n",
    "    \n",
    "    def cost_function_prime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        #delta3 = np.multiply(-(y - self.yHat), self.sigmoid_deriv(self.z3))\n",
    "        delta3 = (self.cost).delta(self.z3, self.yHat, y)\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.multiply(np.dot(delta3, self.W2.T), sigmoid_deriv(self.z2))\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "        \n",
    "        # deltas are for bias term.\n",
    "        return delta2, delta3, dJdW1, dJdW2\n",
    "    \n",
    "    def train_mini_batch(self, X, y, rate, L2):\n",
    "        \"\"\" Train the network on a mini-batch \"\"\"\n",
    "        n = len(y)\n",
    "        self.delta2, self.delta3, self.dJdW1, self.dJdW2 = self.cost_function_prime(X, y)\n",
    "        self.bias2 -= (rate)*np.mean(self.delta2, axis=0)\n",
    "        self.bias3 -= (rate)*np.mean(self.delta3, axis=0)\n",
    "        self.W1 -= (rate/n)*self.dJdW1 - rate*L2*self.W1\n",
    "        self.W2 -= (rate/n)*self.dJdW2 - rate*L2*self.W2\n",
    "        \n",
    "    def stochastic_gradient_descent(self, X, y, number_of_epochs, mini_batch_size, \\\n",
    "                                           rate = 1, L2 = 0.1, test_X = None, test_y = None, lst=[]):\n",
    "        \"\"\" Train the network using the stochastic gradient descent method. \"\"\"\n",
    "        for epoch in range(number_of_epochs):\n",
    "            indexes = np.random.shuffle(np.arange(len(y)))\n",
    "            X_train = X[indexes]\n",
    "            y_train = y[indexes]\n",
    "            batches = [(X[x:x+mini_batch_size], y[x:x+mini_batch_size]) \\\n",
    "                        for x in np.arange(0, len(y), mini_batch_size)]\n",
    "            \n",
    "            for batch in batches:\n",
    "                self.train_mini_batch( batch[0], batch[1], rate, L2)\n",
    "                \n",
    "            if test_X != None and test_y != None:\n",
    "                result = self.evaluate(test_X, test_y)\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(epoch, result, \\\n",
    "                                                           len(test_y)))\n",
    "                lst.append(float(result) / len(test_y))\n",
    "        return lst\n",
    "                \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        \"\"\" Evaluate performance by counting how many examples in test_data are correctly \n",
    "            evaluated. \"\"\"\n",
    "        count = 0\n",
    "        for i in range(len(test_y)):\n",
    "            answer = np.argmax(test_y[i])\n",
    "            prediction = np.argmax(self.forward(test_X[i]))\n",
    "            count = count + 1 if (answer - prediction) == 0 else count\n",
    "        return count\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\" Save neural network (weights) to a file. \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'b2':self.bias2, 'b3':self.bias3, 'W1':self.W1, 'W2':self.W2}, f )\n",
    "        \n",
    "    def load(self, filename):\n",
    "        \"\"\" Load neural network (weights) from a file. \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Set biases and weights\n",
    "        self.W1 = data['W1']\n",
    "        self.W2 = data['W2']\n",
    "        self.bias2 = data['b2']\n",
    "        self.bias3 = data['b3']\n",
    "    \n",
    "    \"\"\"Numerical checking part. \"\"\"\n",
    "    def getParams(self):\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        # Set W1 and W2 using single parameter vector:\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        \n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize * self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        delta2, delta3, dJdW1, dJdW2 = self.cost_function_prime(X, y)\n",
    "        testPrint(\"dJdW1\", dJdW1.shape)\n",
    "        testPrint(\"dJdW2\", dJdW2.shape)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "    \n",
    "\"\"\"Method for numerical check.\"\"\"\n",
    "def computeNumericalGradient(N, X, y, indexes):\n",
    "    paramsInitial = N.getParams()\n",
    "    numgrad = np.zeros(paramsInitial.shape)\n",
    "    perturb = np.zeros(paramsInitial.shape)\n",
    "\n",
    "    e = 1e-4\n",
    "    for p in indexes:\n",
    "        # Set perturbation error\n",
    "        perturb[p] = e\n",
    "        N.setParams(paramsInitial + perturb)\n",
    "        yHat = N.forward(X)\n",
    "        loss2 = sum((N.cost).fn(yHat, y))\n",
    "        \n",
    "        N.setParams(paramsInitial - perturb)\n",
    "        yHat = N.forward(X)\n",
    "        loss1 = sum((N.cost).fn(yHat, y))\n",
    "        \n",
    "        d = (loss2 - loss1) / (2 * e)\n",
    "        testPrint(\"d\", sum(d))\n",
    "        \n",
    "        numgrad[p] = sum(d)\n",
    "        perturb[p] = 0\n",
    "    N.setParams(paramsInitial)\n",
    "    return numgrad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_data = scipy.io.loadmat(\"./dataset/train.mat\")\n",
    "test_data = scipy.io.loadmat(\"./dataset/test.mat\")\n",
    "# _mat = sklearn.preprocessing.scale(_mat)\n",
    "# test_mat = _mat[5172:]\n",
    "# _mat = _mat[0:5172]\n",
    "\n",
    "# print(test_mat.shape)\n",
    "# print(_mat.shape)\n",
    "# _labels = spamData['training_labels'].reshape(5172)\n",
    "# # \n",
    "# v_index = list(set(np.random.randint(5172, size=1724)))\n",
    "# t_index = list(set(range(5172)) - set(v_index))\n",
    "# v_mat = _mat[v_index]\n",
    "# v_labels = _labels[v_index]\n",
    "# t_mat = _mat[t_index]\n",
    "# t_labels = _labels[t_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.1\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "train_images = train_data['train_images'].transpose(2, 0, 1) # Correct rotation for the data. (60000, 28, 28)\n",
    "test_images = test_data['test_images'] # Correct rotation for the data. (10000, 28, 28)\n",
    "# _mat = np.hstack((np.vstack((train_images, test_images)).reshape(70000, 784), np.ones((70000, 1))))\n",
    "_mat = np.vstack((train_images, test_images)).reshape(70000, 784)\n",
    "_mat = sklearn.preprocessing.scale(_mat)\n",
    "train_mat = _mat[:60000]\n",
    "test_mat = _mat[60000:]\n",
    "_labels = list(train_data['train_labels'].reshape(60000))\n",
    "_labels = np.array([[1 if i == _labels[j] else 0 for i in range(10)] for j in range(60000)])\n",
    "print(_labels.shape)\n",
    "\n",
    "v_index = list(np.random.choice(np.arange(60000), size=10000, replace=False))\n",
    "t_index = list(set(range(60000)) - set(v_index))\n",
    "v_mat = train_mat[v_index]\n",
    "v_labels = _labels[v_index]\n",
    "t_mat = train_mat[t_index]\n",
    "t_labels = _labels[t_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter some training we got test1 and test2 as two weight documents.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\"\"\"\n",
    "After some training we got test1 and test2 as two weight documents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:124: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1024 / 10000\n",
      "Epoch 1: 1024 / 10000\n",
      "Epoch 2: 1081 / 10000\n",
      "Epoch 3: 1399 / 10000\n",
      "Epoch 4: 1781 / 10000\n",
      "Epoch 5: 2117 / 10000\n",
      "Epoch 6: 2337 / 10000\n",
      "Epoch 7: 2572 / 10000\n",
      "Epoch 8: 2744 / 10000\n",
      "Epoch 9: 2871 / 10000\n",
      "Epoch 10: 2986 / 10000\n",
      "Epoch 11: 3084 / 10000\n",
      "Epoch 12: 3178 / 10000\n",
      "Epoch 13: 3284 / 10000\n",
      "Epoch 14: 3379 / 10000\n",
      "Epoch 15: 3485 / 10000\n",
      "Epoch 16: 3562 / 10000\n",
      "Epoch 17: 3647 / 10000\n",
      "Epoch 18: 3740 / 10000\n",
      "Epoch 19: 3821 / 10000\n",
      "Epoch 20: 3889 / 10000\n",
      "Epoch 21: 3962 / 10000\n",
      "Epoch 22: 4036 / 10000\n",
      "Epoch 23: 4108 / 10000\n",
      "Epoch 24: 4167 / 10000\n",
      "Epoch 25: 4258 / 10000\n",
      "Epoch 26: 4334 / 10000\n",
      "Epoch 27: 4400 / 10000\n",
      "Epoch 28: 4478 / 10000\n",
      "Epoch 29: 4540 / 10000\n",
      "Epoch 30: 4604 / 10000\n",
      "Epoch 31: 4676 / 10000\n",
      "Epoch 32: 4736 / 10000\n",
      "Epoch 33: 4805 / 10000\n",
      "Epoch 34: 4859 / 10000\n",
      "Epoch 35: 4928 / 10000\n",
      "Epoch 36: 4998 / 10000\n",
      "Epoch 37: 5064 / 10000\n",
      "Epoch 38: 5140 / 10000\n",
      "Epoch 39: 5213 / 10000\n",
      "Epoch 40: 5288 / 10000\n",
      "Epoch 41: 5361 / 10000\n",
      "Epoch 42: 5443 / 10000\n",
      "Epoch 43: 5520 / 10000\n",
      "Epoch 44: 5603 / 10000\n",
      "Epoch 45: 5665 / 10000\n",
      "Epoch 46: 5755 / 10000\n",
      "Epoch 47: 5812 / 10000\n",
      "Epoch 48: 5884 / 10000\n",
      "Epoch 49: 5949 / 10000\n",
      "Epoch 50: 6012 / 10000\n",
      "Epoch 51: 6094 / 10000\n",
      "Epoch 52: 6175 / 10000\n",
      "Epoch 53: 6249 / 10000\n",
      "Epoch 54: 6304 / 10000\n",
      "Epoch 55: 6357 / 10000\n",
      "Epoch 56: 6443 / 10000\n",
      "Epoch 57: 6489 / 10000\n",
      "Epoch 58: 6557 / 10000\n",
      "Epoch 59: 6627 / 10000\n",
      "Epoch 60: 6685 / 10000\n",
      "Epoch 61: 6733 / 10000\n",
      "Epoch 62: 6786 / 10000\n",
      "Epoch 63: 6831 / 10000\n",
      "Epoch 64: 6905 / 10000\n",
      "Epoch 65: 6974 / 10000\n",
      "Epoch 66: 7034 / 10000\n",
      "Epoch 67: 7081 / 10000\n",
      "Epoch 68: 7135 / 10000\n",
      "Epoch 69: 7190 / 10000\n",
      "Epoch 70: 7236 / 10000\n",
      "Epoch 71: 7294 / 10000\n",
      "Epoch 72: 7338 / 10000\n",
      "Epoch 73: 7373 / 10000\n",
      "Epoch 74: 7429 / 10000\n",
      "Epoch 75: 7474 / 10000\n",
      "Epoch 76: 7514 / 10000\n",
      "Epoch 77: 7561 / 10000\n",
      "Epoch 78: 7603 / 10000\n",
      "Epoch 79: 7646 / 10000\n",
      "Epoch 80: 7684 / 10000\n",
      "Epoch 81: 7728 / 10000\n",
      "Epoch 82: 7766 / 10000\n",
      "Epoch 83: 7807 / 10000\n",
      "Epoch 84: 7842 / 10000\n",
      "Epoch 85: 7875 / 10000\n",
      "Epoch 86: 7908 / 10000\n",
      "Epoch 87: 7941 / 10000\n",
      "Epoch 88: 7974 / 10000\n",
      "Epoch 89: 8003 / 10000\n",
      "Epoch 90: 8035 / 10000\n",
      "Epoch 91: 8072 / 10000\n",
      "Epoch 92: 8110 / 10000\n",
      "Epoch 93: 8142 / 10000\n",
      "Epoch 94: 8177 / 10000\n",
      "Epoch 95: 8202 / 10000\n",
      "Epoch 96: 8236 / 10000\n",
      "Epoch 97: 8267 / 10000\n",
      "Epoch 98: 8298 / 10000\n",
      "Epoch 99: 8327 / 10000\n",
      "Epoch 100: 8352 / 10000\n",
      "Epoch 101: 8377 / 10000\n",
      "Epoch 102: 8408 / 10000\n",
      "Epoch 103: 8432 / 10000\n",
      "Epoch 104: 8455 / 10000\n",
      "Epoch 105: 8482 / 10000\n",
      "Epoch 106: 8503 / 10000\n",
      "Epoch 107: 8527 / 10000\n",
      "Epoch 108: 8553 / 10000\n",
      "Epoch 109: 8567 / 10000\n",
      "Epoch 110: 8592 / 10000\n",
      "Epoch 111: 8611 / 10000\n",
      "Epoch 112: 8639 / 10000\n",
      "Epoch 113: 8664 / 10000\n",
      "Epoch 114: 8688 / 10000\n",
      "Epoch 115: 8704 / 10000\n",
      "Epoch 116: 8728 / 10000\n",
      "Epoch 117: 8762 / 10000\n",
      "Epoch 118: 8779 / 10000\n",
      "Epoch 119: 8795 / 10000\n",
      "Epoch 120: 8817 / 10000\n",
      "Epoch 121: 8834 / 10000\n",
      "Epoch 122: 8853 / 10000\n",
      "Epoch 123: 8874 / 10000\n",
      "Epoch 124: 8894 / 10000\n",
      "Epoch 125: 8909 / 10000\n",
      "Epoch 126: 8922 / 10000\n",
      "Epoch 127: 8937 / 10000\n",
      "Epoch 128: 8951 / 10000\n",
      "Epoch 129: 8958 / 10000\n",
      "Epoch 130: 8971 / 10000\n",
      "Epoch 131: 8982 / 10000\n",
      "Epoch 132: 8993 / 10000\n",
      "Epoch 133: 9011 / 10000\n",
      "Epoch 134: 9029 / 10000\n",
      "Epoch 135: 9039 / 10000\n",
      "Epoch 136: 9051 / 10000\n",
      "Epoch 137: 9064 / 10000\n",
      "Epoch 138: 9074 / 10000\n",
      "Epoch 139: 9085 / 10000\n",
      "Epoch 140: 9098 / 10000\n",
      "Epoch 141: 9100 / 10000\n",
      "Epoch 142: 9109 / 10000\n",
      "Epoch 143: 9116 / 10000\n",
      "Epoch 144: 9122 / 10000\n",
      "Epoch 145: 9127 / 10000\n",
      "Epoch 146: 9135 / 10000\n",
      "Epoch 147: 9138 / 10000\n",
      "Epoch 148: 9142 / 10000\n",
      "Epoch 149: 9148 / 10000\n",
      "Epoch 150: 9160 / 10000\n",
      "Epoch 151: 9167 / 10000\n",
      "Epoch 152: 9176 / 10000\n",
      "Epoch 153: 9182 / 10000\n",
      "Epoch 154: 9187 / 10000\n",
      "Epoch 155: 9199 / 10000\n",
      "Epoch 156: 9203 / 10000\n",
      "Epoch 157: 9203 / 10000\n",
      "Epoch 158: 9209 / 10000\n",
      "Epoch 159: 9216 / 10000\n",
      "Epoch 160: 9228 / 10000\n",
      "Epoch 161: 9238 / 10000\n",
      "Epoch 162: 9243 / 10000\n",
      "Epoch 163: 9251 / 10000\n",
      "Epoch 164: 9255 / 10000\n",
      "Epoch 165: 9262 / 10000\n",
      "Epoch 166: 9265 / 10000\n",
      "Epoch 167: 9275 / 10000\n",
      "Epoch 168: 9285 / 10000\n",
      "Epoch 169: 9287 / 10000\n",
      "Epoch 170: 9290 / 10000\n",
      "Epoch 171: 9296 / 10000\n",
      "Epoch 172: 9296 / 10000\n",
      "Epoch 173: 9301 / 10000\n",
      "Epoch 174: 9302 / 10000\n",
      "Epoch 175: 9308 / 10000\n",
      "Epoch 176: 9310 / 10000\n",
      "Epoch 177: 9312 / 10000\n",
      "Epoch 178: 9315 / 10000\n",
      "Epoch 179: 9325 / 10000\n",
      "Epoch 180: 9329 / 10000\n",
      "Epoch 181: 9330 / 10000\n",
      "Epoch 182: 9331 / 10000\n",
      "Epoch 183: 9336 / 10000\n",
      "Epoch 184: 9337 / 10000\n",
      "Epoch 185: 9341 / 10000\n",
      "Epoch 186: 9347 / 10000\n",
      "Epoch 187: 9348 / 10000\n",
      "Epoch 188: 9355 / 10000\n",
      "Epoch 189: 9357 / 10000\n",
      "Epoch 190: 9360 / 10000\n",
      "Epoch 191: 9365 / 10000\n",
      "Epoch 192: 9370 / 10000\n",
      "Epoch 193: 9374 / 10000\n",
      "Epoch 194: 9375 / 10000\n",
      "Epoch 195: 9375 / 10000\n",
      "Epoch 196: 9376 / 10000\n",
      "Epoch 197: 9380 / 10000\n",
      "Epoch 198: 9385 / 10000\n",
      "Epoch 199: 9387 / 10000\n",
      "Epoch 200: 9390 / 10000\n",
      "Epoch 201: 9390 / 10000\n",
      "Epoch 202: 9395 / 10000\n",
      "Epoch 203: 9400 / 10000\n",
      "Epoch 204: 9402 / 10000\n",
      "Epoch 205: 9406 / 10000\n",
      "Epoch 206: 9407 / 10000\n",
      "Epoch 207: 9408 / 10000\n",
      "Epoch 208: 9407 / 10000\n",
      "Epoch 209: 9410 / 10000\n",
      "Epoch 210: 9414 / 10000\n",
      "Epoch 211: 9416 / 10000\n",
      "Epoch 212: 9420 / 10000\n",
      "Epoch 213: 9421 / 10000\n",
      "Epoch 214: 9421 / 10000\n",
      "Epoch 215: 9425 / 10000\n",
      "Epoch 216: 9426 / 10000\n",
      "Epoch 217: 9428 / 10000\n",
      "Epoch 218: 9431 / 10000\n",
      "Epoch 219: 9433 / 10000\n",
      "Epoch 220: 9436 / 10000\n",
      "Epoch 221: 9438 / 10000\n",
      "Epoch 222: 9439 / 10000\n",
      "Epoch 223: 9438 / 10000\n",
      "Epoch 224: 9439 / 10000\n",
      "Epoch 225: 9439 / 10000\n",
      "Epoch 226: 9441 / 10000\n",
      "Epoch 227: 9443 / 10000\n",
      "Epoch 228: 9445 / 10000\n",
      "Epoch 229: 9446 / 10000\n",
      "Epoch 230: 9447 / 10000\n",
      "Epoch 231: 9448 / 10000\n",
      "Epoch 232: 9451 / 10000\n",
      "Epoch 233: 9451 / 10000\n",
      "Epoch 234: 9453 / 10000\n",
      "Epoch 235: 9455 / 10000\n",
      "Epoch 236: 9458 / 10000\n",
      "Epoch 237: 9462 / 10000\n",
      "Epoch 238: 9466 / 10000\n",
      "Epoch 239: 9467 / 10000\n",
      "Epoch 240: 9468 / 10000\n",
      "Epoch 241: 9470 / 10000\n",
      "Epoch 242: 9471 / 10000\n",
      "Epoch 243: 9471 / 10000\n",
      "Epoch 244: 9472 / 10000\n",
      "Epoch 245: 9472 / 10000\n",
      "Epoch 246: 9470 / 10000\n",
      "Epoch 247: 9470 / 10000\n",
      "Epoch 248: 9470 / 10000\n",
      "Epoch 249: 9471 / 10000\n",
      "Epoch 250: 9474 / 10000\n",
      "Epoch 251: 9473 / 10000\n",
      "Epoch 252: 9475 / 10000\n",
      "Epoch 253: 9477 / 10000\n",
      "Epoch 254: 9478 / 10000\n",
      "Epoch 255: 9481 / 10000\n",
      "Epoch 256: 9483 / 10000\n",
      "Epoch 257: 9483 / 10000\n",
      "Epoch 258: 9484 / 10000\n",
      "Epoch 259: 9485 / 10000\n",
      "Epoch 260: 9484 / 10000\n",
      "Epoch 261: 9485 / 10000\n",
      "Epoch 262: 9486 / 10000\n",
      "Epoch 263: 9486 / 10000\n",
      "Epoch 264: 9485 / 10000\n",
      "Epoch 265: 9487 / 10000\n",
      "Epoch 266: 9488 / 10000\n",
      "Epoch 267: 9489 / 10000\n",
      "Epoch 268: 9490 / 10000\n",
      "Epoch 269: 9491 / 10000\n",
      "Epoch 270: 9494 / 10000\n",
      "Epoch 271: 9494 / 10000\n",
      "Epoch 272: 9494 / 10000\n",
      "Epoch 273: 9495 / 10000\n",
      "Epoch 274: 9495 / 10000\n",
      "Epoch 275: 9495 / 10000\n",
      "Epoch 276: 9496 / 10000\n",
      "Epoch 277: 9497 / 10000\n",
      "Epoch 278: 9496 / 10000\n",
      "Epoch 279: 9497 / 10000\n",
      "Epoch 280: 9498 / 10000\n",
      "Epoch 281: 9500 / 10000\n",
      "Epoch 282: 9499 / 10000\n",
      "Epoch 283: 9500 / 10000\n",
      "Epoch 284: 9499 / 10000\n",
      "Epoch 285: 9501 / 10000\n",
      "Epoch 286: 9502 / 10000\n",
      "Epoch 287: 9503 / 10000\n",
      "Epoch 288: 9504 / 10000\n",
      "Epoch 289: 9504 / 10000\n",
      "Epoch 290: 9504 / 10000\n",
      "Epoch 291: 9506 / 10000\n",
      "Epoch 292: 9506 / 10000\n",
      "Epoch 293: 9508 / 10000\n",
      "Epoch 294: 9508 / 10000\n",
      "Epoch 295: 9508 / 10000\n",
      "Epoch 296: 9508 / 10000\n",
      "Epoch 297: 9509 / 10000\n",
      "Epoch 298: 9509 / 10000\n",
      "Epoch 299: 9509 / 10000\n"
     ]
    }
   ],
   "source": [
    "NN1 = Network()\n",
    "np.random.seed(None)\n",
    "error_lst = NN1.stochastic_gradient_descent(t_mat, t_labels, 300, 10, 0.01, 0.001/len(t_labels), \\\n",
    "                                   test_X = v_mat, test_y = v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:124: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9531 / 10000\n",
      "Epoch 1: 9531 / 10000\n",
      "Epoch 2: 9531 / 10000\n",
      "Epoch 3: 9530 / 10000\n",
      "Epoch 4: 9530 / 10000\n",
      "Epoch 5: 9532 / 10000\n",
      "Epoch 6: 9532 / 10000\n",
      "Epoch 7: 9532 / 10000\n",
      "Epoch 8: 9532 / 10000\n",
      "Epoch 9: 9532 / 10000\n",
      "Epoch 10: 9532 / 10000\n",
      "Epoch 11: 9532 / 10000\n",
      "Epoch 12: 9532 / 10000\n",
      "Epoch 13: 9533 / 10000\n",
      "Epoch 14: 9534 / 10000\n",
      "Epoch 15: 9534 / 10000\n",
      "Epoch 16: 9534 / 10000\n",
      "Epoch 17: 9534 / 10000\n",
      "Epoch 18: 9534 / 10000\n",
      "Epoch 19: 9534 / 10000\n",
      "Epoch 20: 9534 / 10000\n",
      "Epoch 21: 9534 / 10000\n",
      "Epoch 22: 9535 / 10000\n",
      "Epoch 23: 9535 / 10000\n",
      "Epoch 24: 9535 / 10000\n",
      "Epoch 25: 9536 / 10000\n",
      "Epoch 26: 9536 / 10000\n",
      "Epoch 27: 9536 / 10000\n",
      "Epoch 28: 9536 / 10000\n",
      "Epoch 29: 9536 / 10000\n",
      "Epoch 30: 9536 / 10000\n",
      "Epoch 31: 9536 / 10000\n",
      "Epoch 32: 9536 / 10000\n",
      "Epoch 33: 9537 / 10000\n",
      "Epoch 34: 9538 / 10000\n",
      "Epoch 35: 9538 / 10000\n",
      "Epoch 36: 9538 / 10000\n",
      "Epoch 37: 9537 / 10000\n",
      "Epoch 38: 9538 / 10000\n",
      "Epoch 39: 9538 / 10000\n",
      "Epoch 40: 9538 / 10000\n",
      "Epoch 41: 9538 / 10000\n",
      "Epoch 42: 9538 / 10000\n",
      "Epoch 43: 9538 / 10000\n",
      "Epoch 44: 9538 / 10000\n",
      "Epoch 45: 9538 / 10000\n",
      "Epoch 46: 9539 / 10000\n",
      "Epoch 47: 9539 / 10000\n",
      "Epoch 48: 9539 / 10000\n",
      "Epoch 49: 9539 / 10000\n"
     ]
    }
   ],
   "source": [
    "error_lst2 = NN1.stochastic_gradient_descent(t_mat, t_labels, 50, 10, 0.005, 0.001/len(t_labels), \\\n",
    "                                    test_X = v_mat, test_y = v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRURK3P8Dy6RSQ24T8PEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113a6c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(map(lambda k:1-k, error_lst2)))\n",
    "NN1.save(\"test2\")\n",
    "NN.load(\"FinalCell96Ver1\")\n",
    "yHat = NN.forward(v_mat)\n",
    "# predicted = np.argmax(yHat, axis = 1)\n",
    "# expected = np.argmax(v_labels, axis = 1)\n",
    "# s = metrics.accuracy_score(expected, predicted)\n",
    "# m = metrics.confusion_matrix(expected, predicted)\n",
    "# print(s)\n",
    "# print(m)\n",
    "prediction = np.argmax(NN.forward(test_mat), axis=1)\n",
    "import csv\n",
    "with open('predicted_digies.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, lineterminator = '\\n')\n",
    "    for i, content in enumerate(prediction):\n",
    "        writer.writerow([i+1, content])\n",
    "# NN1.stochastic_gradient_descent(t_mat, t_labels, 250, 100, 0.05, 0.005, \\\n",
    "#                                    test_X = v_mat, test_y = v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50826, 200) (50826, 10) (784, 200) (200, 10)\n",
      "dJdW1(784, 200)\n",
      "dJdW2(200, 10)\n",
      "d-14.590899957056536\n",
      "d2.5801350147958146\n",
      "d-12.756509640894365\n",
      "d0.10754162076409557\n",
      "d1.2244919344084337\n",
      "d-13.09003203004977\n",
      "d-0.6733428722327517\n",
      "d12.701527900844667\n",
      "d5.067685081030504\n",
      "d16.610999125532544\n",
      "(10,)\n",
      "(10,)\n",
      "[-14.59089841   2.58013447 -12.75650932   0.1075408    1.22449189\n",
      " -13.09003461  -0.67334359  12.70152759   5.0676839   16.61099764]\n",
      "[-14.59089996   2.58013501 -12.75650964   0.10754162   1.22449193\n",
      " -13.09003203  -0.67334287  12.7015279    5.06768508  16.61099913]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing numerical Gradient.\"\"\"\n",
    "NN = Network()\n",
    "# yHat = NN.forward(t_mat)\n",
    "grad = NN.computeGradients(t_mat, t_labels)\n",
    "indexes = np.random.randint(low=30000, high=150000, size=10)\n",
    "numgrad = computeNumericalGradient(NN, t_mat, t_labels, indexes)\n",
    "print(grad[indexes].shape)\n",
    "print(numgrad[indexes].shape)\n",
    "print(grad[indexes])\n",
    "print(numgrad[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both grad and numgrad are the same. \n",
    "Here we randomly choose some axis and compute their direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Testing Box.\"\"\"\n",
    "NN = Network()\n",
    "# yHat = NN.forward(t_mat)\n",
    "# dJdW1, dJdW2 = NN.cost_function_prime(t_mat, t_labels)\n",
    "# cost = NN.cost.fn(yHat, t_labels)\n",
    "# print(sum(cost))\n",
    "# scalar = 0.001\n",
    "# predicted = np.argmax(yHat, axis = 1)\n",
    "# expected = np.argmax(t_labels, axis = 1)\n",
    "# s = metrics.accuracy_score(expected, predicted)\n",
    "# m = metrics.confusion_matrix(expected, predicted)\n",
    "# print(s)\n",
    "# print(m)\n",
    "# NN.W1 = NN.W1 + scalar * dJdW1\n",
    "# NN.W2 = NN.W2 + scalar * dJdW2\n",
    "# yHat = NN.forward(t_mat)\n",
    "# cost = NN.cost.fn(yHat, t_labels)\n",
    "# print(sum(cost))\n",
    "# predicted = np.argmax(yHat, axis = 1)\n",
    "# expected = np.argmax(t_labels, axis = 1)\n",
    "# s = metrics.accuracy_score(expected, predicted)\n",
    "# m = metrics.confusion_matrix(expected, predicted)\n",
    "# print(s)\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is the upgraded Neural Network which uses convoluted layers and different tuning values.\n",
    "Code below this point is largely experimental, and not meant to be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testPrint(content, arg):\n",
    "    print(content+\"{0}\".format(arg))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Apply sigmoid activation function\n",
    "    # Support\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "class QuadraticCost:\n",
    "    \"\"\" Cost functions for quadratic cost. \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(activations, targets):\n",
    "        \"\"\" Evaluate quadratic cost. \"\"\"\n",
    "        return 0.5 * (activations - targets)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def fn_deriv(activations, targets):\n",
    "        \"\"\" Evaluate derivative of quadratic cost. \"\"\"\n",
    "        return activations - targets\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(inputs, activations, targets):\n",
    "        \"\"\" Compute the delta error at the output layer for the quadratic cost. \"\"\"\n",
    "        return (activations - targets)*sigmoid_deriv(inputs)\n",
    "    \n",
    "class CrossEntropyCost:\n",
    "    \"\"\" Cost functions for cross entropy cost. \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(activations, targets):\n",
    "        \"\"\" Evaluate cross entropy cost. \"\"\"\n",
    "        # The np.nan_to_num function ensures that np.log(0) evaluates to 0 instead of nan.\n",
    "        return -np.nan_to_num(targets*np.log(activations) + \\\n",
    "                                                  (1-targets)*np.log(1 - activations))  \n",
    "    @staticmethod\n",
    "    def fn_deriv(activations, targets):\n",
    "        \"\"\" Evaluate the derivative of the cross entropy cost. \"\"\"\n",
    "        return -np.nan_to_num(targets/activations - (1-targets)/(1-activations))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(inputs, activations, targets):\n",
    "        \"\"\" Compute the delta error at the output layer for the cross entropy cost. \"\"\"\n",
    "        return (activations-targets)\n",
    "\n",
    "class ComplexNetwork(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Synapse: take a value and multiply by its weight.\n",
    "    Neuron: z = x1 + x2 + x3...\n",
    "            a = 1/(1 + e^-z)\n",
    "    \"\"\"\n",
    "    def __init__(self, cost=QuadraticCost):\n",
    "        self.inputLayerSize = 784\n",
    "        self.outputLayerSize = 10\n",
    "        self.hiddenLayer1Size = 1200\n",
    "        self.hiddenLayer2Size = 1200\n",
    "        \n",
    "        self.num_of_layers = 2\n",
    "        \n",
    "        self.cost = cost\n",
    "\n",
    "        self.W1 = 0.1 * np.random.randn(self.inputLayerSize, self.hiddenLayer1Size)\n",
    "        self.W2 = 0.1 * np.random.randn(self.hiddenLayer1Size, self.hiddenLayer2Size)\n",
    "        self.W3 = 0.1 * np.random.randn(self.hiddenLayer2Size, self.outputLayerSize)\n",
    "        self.bias2 = 0.1 * np.random.randn(self.hiddenLayer1Size)\n",
    "        self.bias3 = 0.1 * np.random.randn(self.hiddenLayer2Size)\n",
    "        self.bias4 = 0.1 * np.random.randn(self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X, dropout=False):\n",
    "        \"\"\"\n",
    "        Propagate multiple input through matrix\n",
    "        Do matrix multiplication by input layer and weight on synapse.\n",
    "\n",
    "        X * W(1) = Z(2)\n",
    "        Z is row(example)(784) * col(hidden)(200)\n",
    "\n",
    "        a(2) = s(Z(2))\n",
    "        a is 200 x 200\n",
    "\n",
    "        z(3) = a(2)W(2)\n",
    "        z(3) is 200 * 10\n",
    "\n",
    "        y = f(z(3))\n",
    "        \"\"\"\n",
    "        self.z2 = np.dot(X, self.W1) + self.bias2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        if dropout:\n",
    "            m2 = np.random.binomial(1, 0.5, size=z2.shape)\n",
    "        else:\n",
    "            m2 = 1\n",
    "        self.a2 *= m2\n",
    "        self.z3 = np.dot(self.a2, self.W2) + self.bias3\n",
    "        self.a3 = sigmoid(self.z3)\n",
    "        if dropout:\n",
    "            m3 = np.random.binomial(1, 0.5, size=z3.shape)\n",
    "        else:\n",
    "            m3 = 1\n",
    "        self.z4 = np.dot(self.a3, self.W3) + self.bias4\n",
    "        yHat = sigmoid(self.z4)\n",
    "        return yHat,m2,m3\n",
    "    \n",
    "    def cost_function_prime(self, X, y, m2=1, m3=1):\n",
    "        self.yHat,m2,m3 = self.forward(X)\n",
    "        \n",
    "        #delta3 = np.multiply(-(y - self.yHat), self.sigmoid_deriv(self.z3))\n",
    "        delta4 = (self.cost).delta(self.z4, self.yHat, y)\n",
    "        dJdW3 = np.dot(self.a3.T, delta4)\n",
    "        \n",
    "        delta3 = np.multiply(np.dot(delta4, self.W3.T), sigmoid_deriv(self.z3)) * m3\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.multiply(np.dot(delta3, self.W2.T), sigmoid_deriv(self.z2)) * m2\n",
    "        dJdW1 = np.dot(X.T, delta2)\n",
    "        \n",
    "        # deltas are for bias term.\n",
    "        return delta2, delta3, delta4, dJdW1, dJdW2, dJdW3\n",
    "    \n",
    "    def train_mini_batch(self, X, y, rate, L2, momentum):\n",
    "        \"\"\" Train the network on a mini-batch \"\"\"\n",
    "        n = len(y)\n",
    "        self.delta2, self.delta3, self.delta4, self.dJdW1, self.dJdW2, self.dJdW3 = self.cost_function_prime(X, y)\n",
    "        self.bias2 -= (rate)*np.mean(self.delta2, axis=0)\n",
    "        self.bias3 -= (rate)*np.mean(self.delta3, axis=0)\n",
    "        self.bias4 -= (rate)*np.mean(self.delta4, axis=0)\n",
    "        self.W1 -= (rate/n)*self.dJdW1*(1-momentum) - rate*L2*self.W1*momentum\n",
    "        self.W2 -= (rate/n)*self.dJdW2*(1-momentum) - rate*L2*self.W2*momentum\n",
    "        self.W3 -= (rate/n)*self.dJdW3*(1-momentum) - rate*L2*self.W3*momentum\n",
    "        \n",
    "    def calculate_momentum(self, iteration, pi=0.5, pf=0.99, T=500.0):\n",
    "        if iteration < T:\n",
    "            return iteration/T*pi + (1-iteration/T)*pf\n",
    "        else:\n",
    "            return pf\n",
    "        \n",
    "    def stochastic_gradient_descent(self, X, y, number_of_epochs, mini_batch_size, \\\n",
    "                                           rate = 10, L2 = 0.1, test_X = None, test_y = None, t=0, filename=\"default\"):\n",
    "        \"\"\" Train the network using the stochastic gradient descent method. \"\"\"\n",
    "        for epoch in range(number_of_epochs):\n",
    "            rate *= 0.998\n",
    "            indexes = np.random.shuffle(np.arange(len(y)))\n",
    "            X_train = X[indexes]\n",
    "            y_train = y[indexes]\n",
    "            batches = [(X[x:x+mini_batch_size], y[x:x+mini_batch_size]) \\\n",
    "                        for x in np.arange(0, len(y), mini_batch_size)]\n",
    "            \n",
    "            momentum = self.calculate_momentum(epoch+t)\n",
    "            for batch in batches:\n",
    "                self.train_mini_batch( batch[0], batch[1], rate, L2, momentum)\n",
    "                \n",
    "            if test_X != None and test_y != None:\n",
    "                result = self.evaluate(test_X, test_y)\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(epoch, result, \\\n",
    "                                                           len(test_y)))\n",
    "            if epoch % 10 == 0:\n",
    "                self.save(filename)\n",
    "        return rate\n",
    "                \n",
    "    def evaluate(self, test_X, test_y):\n",
    "        \"\"\" Evaluate performance by counting how many examples in test_data are correctly \n",
    "            evaluated. \"\"\"\n",
    "        count = 0\n",
    "        for i in range(len(test_y)):\n",
    "            answer = np.argmax(test_y[i])\n",
    "            prediction = np.argmax(self.forward(test_X[i])[0])\n",
    "            count = count + 1 if (answer - prediction) == 0 else count\n",
    "        return count\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\" Save neural network (weights) to a file. \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'b2':self.bias2, 'b3':self.bias3, 'b4':self.bias4, 'W1':self.W1, 'W2':self.W2, 'W3':self.W3}, f )\n",
    "        \n",
    "    def load(self, filename):\n",
    "        \"\"\" Load neural network (weights) from a file. \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Set biases and weights\n",
    "        self.W1 = data['W1']\n",
    "        self.W2 = data['W2']\n",
    "        self.W3 = data['W3']\n",
    "        self.bias2 = data['b2']\n",
    "        self.bias3 = data['b3']\n",
    "        self.bias4 = data['b4']\n",
    "    \n",
    "#     \"\"\"Numerical checking part. \"\"\"\n",
    "#     def getParams(self):\n",
    "#         params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "#         return params\n",
    "    \n",
    "#     def setParams(self, params):\n",
    "#         # Set W1 and W2 using single parameter vector:\n",
    "#         W1_start = 0\n",
    "#         W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        \n",
    "#         self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize, self.hiddenLayerSize))\n",
    "#         W2_end = W1_end + self.hiddenLayerSize * self.outputLayerSize\n",
    "#         self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "#     def computeGradients(self, X, y):\n",
    "#         delta2, delta3, delta4, dJdW1, dJdW2, dJdW3 = self.cost_function_prime(X, y)\n",
    "#         return np.concatenate((dJdW1.ravel(), dJdW2.ravel(), dJdW3.ravel()))\n",
    "    \n",
    "# \"\"\"Method for numerical check.\"\"\"\n",
    "# def computeNumericalGradient(N, X, y, indexes):\n",
    "#     paramsInitial = N.getParams()\n",
    "#     numgrad = np.zeros(paramsInitial.shape)\n",
    "#     perturb = np.zeros(paramsInitial.shape)\n",
    "\n",
    "#     e = 1e-4\n",
    "#     for p in indexes:\n",
    "#         # Set perturbation error\n",
    "#         perturb[p] = e\n",
    "#         N.setParams(paramsInitial + perturb)\n",
    "#         yHat = N.forward(X)\n",
    "#         loss2 = sum((N.cost).fn(yHat, y))\n",
    "        \n",
    "#         N.setParams(paramsInitial - perturb)\n",
    "#         yHat = N.forward(X)\n",
    "#         loss1 = sum((N.cost).fn(yHat, y))\n",
    "        \n",
    "#         d = (loss2 - loss1) / (2 * e)\n",
    "#         testPrint(\"d\", sum(d))\n",
    "        \n",
    "#         numgrad[p] = sum(d)\n",
    "#         perturb[p] = 0\n",
    "#     N.setParams(paramsInitial)\n",
    "#     return numgrad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:151: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1009 / 10000\n",
      "Epoch 1: 1009 / 10000\n",
      "Epoch 2: 1009 / 10000\n",
      "Epoch 3: 1009 / 10000\n",
      "Epoch 4: 1015 / 10000\n",
      "Epoch 5: 1033 / 10000\n",
      "Epoch 6: 1070 / 10000\n",
      "Epoch 7: 1115 / 10000\n",
      "Epoch 8: 1164 / 10000\n",
      "Epoch 9: 1195 / 10000\n",
      "Epoch 10: 1231 / 10000\n",
      "Epoch 11: 1277 / 10000\n",
      "Epoch 12: 1326 / 10000\n",
      "Epoch 13: 1368 / 10000\n",
      "Epoch 14: 1409 / 10000\n",
      "Epoch 15: 1455 / 10000\n",
      "Epoch 16: 1517 / 10000\n",
      "Epoch 17: 1558 / 10000\n",
      "Epoch 18: 1616 / 10000\n",
      "Epoch 19: 1671 / 10000\n",
      "Epoch 20: 1723 / 10000\n",
      "Epoch 21: 1779 / 10000\n",
      "Epoch 22: 1827 / 10000\n",
      "Epoch 23: 1883 / 10000\n",
      "Epoch 24: 1920 / 10000\n",
      "Epoch 25: 1962 / 10000\n",
      "Epoch 26: 2003 / 10000\n",
      "Epoch 27: 2037 / 10000\n",
      "Epoch 28: 2074 / 10000\n",
      "Epoch 29: 2124 / 10000\n",
      "Epoch 30: 2168 / 10000\n",
      "Epoch 31: 2204 / 10000\n",
      "Epoch 32: 2230 / 10000\n",
      "Epoch 33: 2271 / 10000\n",
      "Epoch 34: 2304 / 10000\n",
      "Epoch 35: 2321 / 10000\n",
      "Epoch 36: 2354 / 10000\n",
      "Epoch 37: 2387 / 10000\n",
      "Epoch 38: 2411 / 10000\n",
      "Epoch 39: 2438 / 10000\n",
      "Epoch 40: 2455 / 10000\n",
      "Epoch 41: 2478 / 10000\n",
      "Epoch 42: 2503 / 10000\n",
      "Epoch 43: 2510 / 10000\n",
      "Epoch 44: 2534 / 10000\n",
      "Epoch 45: 2557 / 10000\n",
      "Epoch 46: 2586 / 10000\n",
      "Epoch 47: 2606 / 10000\n",
      "Epoch 48: 2619 / 10000\n",
      "Epoch 49: 2641 / 10000\n",
      "Epoch 50: 2664 / 10000\n",
      "Epoch 51: 2692 / 10000\n",
      "Epoch 52: 2708 / 10000\n",
      "Epoch 53: 2729 / 10000\n",
      "Epoch 54: 2753 / 10000\n",
      "Epoch 55: 2780 / 10000\n",
      "Epoch 56: 2810 / 10000\n",
      "Epoch 57: 2833 / 10000\n",
      "Epoch 58: 2854 / 10000\n",
      "Epoch 59: 2881 / 10000\n",
      "Epoch 60: 2907 / 10000\n",
      "Epoch 61: 2925 / 10000\n",
      "Epoch 62: 2948 / 10000\n",
      "Epoch 63: 2961 / 10000\n",
      "Epoch 64: 2991 / 10000\n",
      "Epoch 65: 3010 / 10000\n",
      "Epoch 66: 3038 / 10000\n",
      "Epoch 67: 3070 / 10000\n",
      "Epoch 68: 3090 / 10000\n",
      "Epoch 69: 3123 / 10000\n",
      "Epoch 70: 3142 / 10000\n",
      "Epoch 71: 3166 / 10000\n",
      "Epoch 72: 3188 / 10000\n",
      "Epoch 73: 3222 / 10000\n",
      "Epoch 74: 3247 / 10000\n",
      "Epoch 75: 3272 / 10000\n",
      "Epoch 76: 3299 / 10000\n",
      "Epoch 77: 3329 / 10000\n",
      "Epoch 78: 3371 / 10000\n",
      "Epoch 79: 3418 / 10000\n",
      "Epoch 80: 3456 / 10000\n",
      "Epoch 81: 3494 / 10000\n",
      "Epoch 82: 3523 / 10000\n",
      "Epoch 83: 3554 / 10000\n",
      "Epoch 84: 3582 / 10000\n",
      "Epoch 85: 3612 / 10000\n",
      "Epoch 86: 3645 / 10000\n",
      "Epoch 87: 3681 / 10000\n",
      "Epoch 88: 3713 / 10000\n",
      "Epoch 89: 3751 / 10000\n",
      "Epoch 90: 3791 / 10000\n",
      "Epoch 91: 3841 / 10000\n",
      "Epoch 92: 3883 / 10000\n",
      "Epoch 93: 3932 / 10000\n",
      "Epoch 94: 3980 / 10000\n",
      "Epoch 95: 4026 / 10000\n",
      "Epoch 96: 4052 / 10000\n",
      "Epoch 97: 4084 / 10000\n",
      "Epoch 98: 4117 / 10000\n",
      "Epoch 99: 4148 / 10000\n",
      "Epoch 100: 4173 / 10000\n",
      "Epoch 101: 4211 / 10000\n",
      "Epoch 102: 4253 / 10000\n",
      "Epoch 103: 4295 / 10000\n",
      "Epoch 104: 4338 / 10000\n",
      "Epoch 105: 4391 / 10000\n",
      "Epoch 106: 4452 / 10000\n",
      "Epoch 107: 4531 / 10000\n",
      "Epoch 108: 4577 / 10000\n",
      "Epoch 109: 4652 / 10000\n",
      "Epoch 110: 4712 / 10000\n",
      "Epoch 111: 4772 / 10000\n",
      "Epoch 112: 4833 / 10000\n",
      "Epoch 113: 4893 / 10000\n",
      "Epoch 114: 4942 / 10000\n",
      "Epoch 115: 4993 / 10000\n",
      "Epoch 116: 5047 / 10000\n",
      "Epoch 117: 5104 / 10000\n",
      "Epoch 118: 5167 / 10000\n",
      "Epoch 119: 5237 / 10000\n",
      "Epoch 120: 5235 / 10000\n",
      "Epoch 121: 5117 / 10000\n",
      "Epoch 122: 5215 / 10000\n",
      "Epoch 123: 5239 / 10000\n",
      "Epoch 124: 5332 / 10000\n",
      "Epoch 125: 5468 / 10000\n",
      "Epoch 126: 5549 / 10000\n",
      "Epoch 127: 5681 / 10000\n",
      "Epoch 128: 5808 / 10000\n",
      "Epoch 129: 5881 / 10000\n",
      "Epoch 130: 5988 / 10000\n",
      "Epoch 131: 6021 / 10000\n",
      "Epoch 132: 6028 / 10000\n",
      "Epoch 133: 6073 / 10000\n",
      "Epoch 134: 6118 / 10000\n",
      "Epoch 135: 6150 / 10000\n",
      "Epoch 136: 6207 / 10000\n",
      "Epoch 137: 6272 / 10000\n",
      "Epoch 138: 6278 / 10000\n",
      "Epoch 139: 6361 / 10000\n",
      "Epoch 140: 6405 / 10000\n",
      "Epoch 141: 6441 / 10000\n",
      "Epoch 142: 6516 / 10000\n",
      "Epoch 143: 6530 / 10000\n",
      "Epoch 144: 6598 / 10000\n",
      "Epoch 145: 6679 / 10000\n",
      "Epoch 146: 6719 / 10000\n",
      "Epoch 147: 6745 / 10000\n",
      "Epoch 148: 6669 / 10000\n",
      "Epoch 149: 6771 / 10000\n",
      "Epoch 150: 6824 / 10000\n",
      "Epoch 151: 6874 / 10000\n",
      "Epoch 152: 6931 / 10000\n",
      "Epoch 153: 6950 / 10000\n",
      "Epoch 154: 6922 / 10000\n",
      "Epoch 155: 7061 / 10000\n",
      "Epoch 156: 7059 / 10000\n",
      "Epoch 157: 7152 / 10000\n",
      "Epoch 158: 7139 / 10000\n",
      "Epoch 159: 7230 / 10000\n",
      "Epoch 160: 7262 / 10000\n",
      "Epoch 161: 7295 / 10000\n",
      "Epoch 162: 7330 / 10000\n",
      "Epoch 163: 7353 / 10000\n",
      "Epoch 164: 7373 / 10000\n",
      "Epoch 165: 7400 / 10000\n",
      "Epoch 166: 7422 / 10000\n",
      "Epoch 167: 7356 / 10000\n",
      "Epoch 168: 7301 / 10000\n",
      "Epoch 169: 7466 / 10000\n",
      "Epoch 170: 7548 / 10000\n",
      "Epoch 171: 7528 / 10000\n",
      "Epoch 172: 6639 / 10000\n",
      "Epoch 173: 7644 / 10000\n",
      "Epoch 174: 7300 / 10000\n",
      "Epoch 175: 7703 / 10000\n",
      "Epoch 176: 7718 / 10000\n",
      "Epoch 177: 7774 / 10000\n",
      "Epoch 178: 7807 / 10000\n",
      "Epoch 179: 7822 / 10000\n",
      "Epoch 180: 7867 / 10000\n",
      "Epoch 181: 7924 / 10000\n",
      "Epoch 182: 7513 / 10000\n",
      "Epoch 183: 7925 / 10000\n",
      "Epoch 184: 7958 / 10000\n",
      "Epoch 185: 8034 / 10000\n",
      "Epoch 186: 8055 / 10000\n",
      "Epoch 187: 7900 / 10000\n",
      "Epoch 188: 8082 / 10000\n",
      "Epoch 189: 8121 / 10000\n",
      "Epoch 190: 8136 / 10000\n",
      "Epoch 191: 8188 / 10000\n",
      "Epoch 192: 8214 / 10000\n",
      "Epoch 193: 8199 / 10000\n",
      "Epoch 194: 8247 / 10000\n",
      "Epoch 195: 8265 / 10000\n",
      "Epoch 196: 8253 / 10000\n",
      "Epoch 197: 8353 / 10000\n",
      "Epoch 198: 8272 / 10000\n",
      "Epoch 199: 8388 / 10000\n",
      "Epoch 200: 8413 / 10000\n",
      "Epoch 201: 8456 / 10000\n",
      "Epoch 202: 8477 / 10000\n",
      "Epoch 203: 8504 / 10000\n",
      "Epoch 204: 8518 / 10000\n",
      "Epoch 205: 8537 / 10000\n",
      "Epoch 206: 8565 / 10000\n",
      "Epoch 207: 8572 / 10000\n",
      "Epoch 208: 8528 / 10000\n",
      "Epoch 209: 8578 / 10000\n",
      "Epoch 210: 8584 / 10000\n",
      "Epoch 211: 8646 / 10000\n",
      "Epoch 212: 8608 / 10000\n",
      "Epoch 213: 8703 / 10000\n",
      "Epoch 214: 8726 / 10000\n",
      "Epoch 215: 8730 / 10000\n",
      "Epoch 216: 8756 / 10000\n",
      "Epoch 217: 8756 / 10000\n",
      "Epoch 218: 8771 / 10000\n",
      "Epoch 219: 8783 / 10000\n",
      "Epoch 220: 8803 / 10000\n",
      "Epoch 221: 8808 / 10000\n",
      "Epoch 222: 8827 / 10000\n",
      "Epoch 223: 8832 / 10000\n",
      "Epoch 224: 8856 / 10000\n",
      "Epoch 225: 8877 / 10000\n",
      "Epoch 226: 8886 / 10000\n",
      "Epoch 227: 8910 / 10000\n",
      "Epoch 228: 8924 / 10000\n",
      "Epoch 229: 8935 / 10000\n",
      "Epoch 230: 8944 / 10000\n",
      "Epoch 231: 8954 / 10000\n",
      "Epoch 232: 8969 / 10000\n",
      "Epoch 233: 8977 / 10000\n",
      "Epoch 234: 8989 / 10000\n",
      "Epoch 235: 8996 / 10000\n",
      "Epoch 236: 9004 / 10000\n",
      "Epoch 237: 9010 / 10000\n",
      "Epoch 238: 9018 / 10000\n",
      "Epoch 239: 9028 / 10000\n",
      "Epoch 240: 9041 / 10000\n",
      "Epoch 241: 9049 / 10000\n",
      "Epoch 242: 9028 / 10000\n",
      "Epoch 243: 9045 / 10000\n",
      "Epoch 244: 9043 / 10000\n",
      "Epoch 245: 9063 / 10000\n",
      "Epoch 246: 9074 / 10000\n",
      "Epoch 247: 9072 / 10000\n",
      "Epoch 248: 9097 / 10000\n",
      "Epoch 249: 9109 / 10000\n",
      "Epoch 250: 9111 / 10000\n",
      "Epoch 251: 9119 / 10000\n",
      "Epoch 252: 9126 / 10000\n",
      "Epoch 253: 9137 / 10000\n",
      "Epoch 254: 9144 / 10000\n",
      "Epoch 255: 9160 / 10000\n",
      "Epoch 256: 9164 / 10000\n",
      "Epoch 257: 9165 / 10000\n",
      "Epoch 258: 9169 / 10000\n",
      "Epoch 259: 9181 / 10000\n",
      "Epoch 260: 9193 / 10000\n",
      "Epoch 261: 9201 / 10000\n",
      "Epoch 262: 9206 / 10000\n",
      "Epoch 263: 9208 / 10000\n",
      "Epoch 264: 9215 / 10000\n",
      "Epoch 265: 9229 / 10000\n",
      "Epoch 266: 9235 / 10000\n",
      "Epoch 267: 9239 / 10000\n",
      "Epoch 268: 9235 / 10000\n",
      "Epoch 269: 9252 / 10000\n",
      "Epoch 270: 9253 / 10000\n",
      "Epoch 271: 9255 / 10000\n",
      "Epoch 272: 9263 / 10000\n",
      "Epoch 273: 9271 / 10000\n",
      "Epoch 274: 9276 / 10000\n",
      "Epoch 275: 9285 / 10000\n",
      "Epoch 276: 9287 / 10000\n",
      "Epoch 277: 9291 / 10000\n",
      "Epoch 278: 9296 / 10000\n",
      "Epoch 279: 9297 / 10000\n",
      "Epoch 280: 9302 / 10000\n",
      "Epoch 281: 9317 / 10000\n",
      "Epoch 282: 9319 / 10000\n",
      "Epoch 283: 9323 / 10000\n",
      "Epoch 284: 9327 / 10000\n",
      "Epoch 285: 9334 / 10000\n",
      "Epoch 286: 9338 / 10000\n",
      "Epoch 287: 9342 / 10000\n",
      "Epoch 288: 9346 / 10000\n",
      "Epoch 289: 9352 / 10000\n",
      "Epoch 290: 9354 / 10000\n",
      "Epoch 291: 9359 / 10000\n",
      "Epoch 292: 9366 / 10000\n",
      "Epoch 293: 9369 / 10000\n",
      "Epoch 294: 9367 / 10000\n",
      "Epoch 295: 9372 / 10000\n",
      "Epoch 296: 9374 / 10000\n",
      "Epoch 297: 9384 / 10000\n",
      "Epoch 298: 9387 / 10000\n",
      "Epoch 299: 9395 / 10000\n",
      "Epoch 0: 1012 / 10000\n",
      "Epoch 1: 1012 / 10000\n",
      "Epoch 2: 1012 / 10000\n",
      "Epoch 3: 1012 / 10000\n",
      "Epoch 4: 1012 / 10000\n",
      "Epoch 5: 1012 / 10000\n",
      "Epoch 6: 1012 / 10000\n",
      "Epoch 7: 1012 / 10000\n",
      "Epoch 8: 1012 / 10000\n",
      "Epoch 9: 1012 / 10000\n",
      "Epoch 10: 1012 / 10000\n",
      "Epoch 11: 1012 / 10000\n",
      "Epoch 12: 1012 / 10000\n",
      "Epoch 13: 1012 / 10000\n",
      "Epoch 14: 1012 / 10000\n",
      "Epoch 15: 1012 / 10000\n",
      "Epoch 16: 1012 / 10000\n",
      "Epoch 17: 1012 / 10000\n",
      "Epoch 18: 1012 / 10000\n",
      "Epoch 19: 1012 / 10000\n",
      "Epoch 20: 1013 / 10000\n",
      "Epoch 21: 1013 / 10000\n",
      "Epoch 22: 1013 / 10000\n",
      "Epoch 23: 1013 / 10000\n",
      "Epoch 24: 1013 / 10000\n",
      "Epoch 25: 1013 / 10000\n",
      "Epoch 26: 1013 / 10000\n",
      "Epoch 27: 1012 / 10000\n",
      "Epoch 28: 1012 / 10000\n",
      "Epoch 29: 1012 / 10000\n",
      "Epoch 30: 1012 / 10000\n",
      "Epoch 31: 1012 / 10000\n",
      "Epoch 32: 1012 / 10000\n",
      "Epoch 33: 1012 / 10000\n",
      "Epoch 34: 1012 / 10000\n",
      "Epoch 35: 1012 / 10000\n",
      "Epoch 36: 1012 / 10000\n",
      "Epoch 37: 1012 / 10000\n",
      "Epoch 38: 1012 / 10000\n",
      "Epoch 39: 1012 / 10000\n",
      "Epoch 40: 1012 / 10000\n",
      "Epoch 41: 1012 / 10000\n",
      "Epoch 42: 1012 / 10000\n",
      "Epoch 43: 1012 / 10000\n",
      "Epoch 44: 1012 / 10000\n",
      "Epoch 45: 1012 / 10000\n",
      "Epoch 46: 1012 / 10000\n",
      "Epoch 47: 1012 / 10000\n",
      "Epoch 48: 1012 / 10000\n",
      "Epoch 49: 1012 / 10000\n",
      "Epoch 50: 1012 / 10000\n",
      "Epoch 51: 1012 / 10000\n",
      "Epoch 52: 1012 / 10000\n",
      "Epoch 53: 1012 / 10000\n",
      "Epoch 54: 1012 / 10000\n",
      "Epoch 55: 1012 / 10000\n",
      "Epoch 56: 1012 / 10000\n",
      "Epoch 57: 1012 / 10000\n",
      "Epoch 58: 1012 / 10000\n",
      "Epoch 59: 1012 / 10000\n",
      "Epoch 60: 1012 / 10000\n",
      "Epoch 61: 1012 / 10000\n",
      "Epoch 62: 1012 / 10000\n",
      "Epoch 63: 1012 / 10000\n",
      "Epoch 64: 1012 / 10000\n",
      "Epoch 65: 1012 / 10000\n",
      "Epoch 66: 1012 / 10000\n",
      "Epoch 67: 1012 / 10000\n",
      "Epoch 68: 1012 / 10000\n",
      "Epoch 69: 1012 / 10000\n",
      "Epoch 70: 1012 / 10000\n",
      "Epoch 71: 1012 / 10000\n",
      "Epoch 72: 1012 / 10000\n",
      "Epoch 73: 1013 / 10000\n",
      "Epoch 74: 1012 / 10000\n",
      "Epoch 75: 1012 / 10000\n",
      "Epoch 76: 1012 / 10000\n",
      "Epoch 77: 1012 / 10000\n",
      "Epoch 78: 1012 / 10000\n",
      "Epoch 79: 1012 / 10000\n",
      "Epoch 80: 1012 / 10000\n",
      "Epoch 81: 1012 / 10000\n",
      "Epoch 82: 1012 / 10000\n",
      "Epoch 83: 1012 / 10000\n",
      "Epoch 84: 1012 / 10000\n",
      "Epoch 85: 1012 / 10000\n",
      "Epoch 86: 1012 / 10000\n",
      "Epoch 87: 1012 / 10000\n",
      "Epoch 88: 1012 / 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-883043768f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComplexNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test2_CNN_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                    \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a739399ef718>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(self, X, y, number_of_epochs, mini_batch_size, rate, L2, test_X, test_y, t, filename)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_momentum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a739399ef718>\u001b[0m in \u001b[0;36mtrain_mini_batch\u001b[0;34m(self, X, y, rate, L2, momentum)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias4\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdJdW1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdJdW2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdJdW3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# np.random.seed(None)\n",
    "# val = 0\n",
    "# NN_lst = []\n",
    "v_index = list(np.random.choice(np.arange(60000), size=10000, replace=False))\n",
    "t_index = list(set(range(60000)) - set(v_index))\n",
    "v_mat = train_mat[v_index]\n",
    "v_labels = _labels[v_index]\n",
    "t_mat = train_mat[t_index]\n",
    "t_labels = _labels[t_index]\n",
    "rate = 10\n",
    "val = 0\n",
    "for i in range(3, 5):\n",
    "    CNN = ComplexNetwork()\n",
    "    file_name = \"test2_CNN_\" + str(i)\n",
    "    rate = CNN.stochastic_gradient_descent(t_mat, t_labels, 300, 100, rate, 0.01/len(t_labels), \\\n",
    "                                   test_X = v_mat, test_y = v_labels, t=val, filename = file_name)\n",
    "    CNN.save(file_name)\n",
    "    val += 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuconghe/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:151: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9428 / 10000\n",
      "Epoch 1: 9437 / 10000\n",
      "Epoch 2: 9441 / 10000\n",
      "Epoch 3: 9444 / 10000\n",
      "Epoch 4: 9447 / 10000\n",
      "Epoch 5: 9450 / 10000\n",
      "Epoch 6: 9451 / 10000\n",
      "Epoch 7: 9453 / 10000\n",
      "Epoch 8: 9458 / 10000\n",
      "Epoch 9: 9466 / 10000\n",
      "Epoch 10: 9469 / 10000\n",
      "Epoch 11: 9471 / 10000\n",
      "Epoch 12: 9472 / 10000\n",
      "Epoch 13: 9472 / 10000\n",
      "Epoch 14: 9475 / 10000\n",
      "Epoch 15: 9480 / 10000\n",
      "Epoch 16: 9480 / 10000\n",
      "Epoch 17: 9483 / 10000\n",
      "Epoch 18: 9485 / 10000\n",
      "Epoch 19: 9487 / 10000\n",
      "Epoch 20: 9491 / 10000\n",
      "Epoch 21: 9493 / 10000\n",
      "Epoch 22: 9494 / 10000\n",
      "Epoch 23: 9497 / 10000\n",
      "Epoch 24: 9498 / 10000\n",
      "Epoch 25: 9500 / 10000\n",
      "Epoch 26: 9500 / 10000\n",
      "Epoch 27: 9499 / 10000\n",
      "Epoch 28: 9501 / 10000\n",
      "Epoch 29: 9503 / 10000\n",
      "Epoch 30: 9504 / 10000\n",
      "Epoch 31: 9505 / 10000\n",
      "Epoch 32: 9509 / 10000\n",
      "Epoch 33: 9511 / 10000\n",
      "Epoch 34: 9514 / 10000\n",
      "Epoch 35: 9518 / 10000\n",
      "Epoch 36: 9520 / 10000\n",
      "Epoch 37: 9521 / 10000\n",
      "Epoch 38: 9523 / 10000\n",
      "Epoch 39: 9524 / 10000\n",
      "Epoch 40: 9528 / 10000\n",
      "Epoch 41: 9527 / 10000\n",
      "Epoch 42: 9530 / 10000\n",
      "Epoch 43: 9530 / 10000\n",
      "Epoch 44: 9533 / 10000\n",
      "Epoch 45: 9533 / 10000\n",
      "Epoch 46: 9534 / 10000\n",
      "Epoch 47: 9534 / 10000\n",
      "Epoch 48: 9537 / 10000\n",
      "Epoch 49: 9537 / 10000\n",
      "Epoch 50: 9537 / 10000\n",
      "Epoch 51: 9541 / 10000\n",
      "Epoch 52: 9543 / 10000\n",
      "Epoch 53: 9545 / 10000\n",
      "Epoch 54: 9547 / 10000\n",
      "Epoch 55: 9550 / 10000\n",
      "Epoch 56: 9553 / 10000\n",
      "Epoch 57: 9553 / 10000\n",
      "Epoch 58: 9555 / 10000\n",
      "Epoch 59: 9556 / 10000\n",
      "Epoch 60: 9557 / 10000\n",
      "Epoch 61: 9557 / 10000\n",
      "Epoch 62: 9560 / 10000\n",
      "Epoch 63: 9561 / 10000\n",
      "Epoch 64: 9561 / 10000\n",
      "Epoch 65: 9563 / 10000\n",
      "Epoch 66: 9564 / 10000\n",
      "Epoch 67: 9565 / 10000\n",
      "Epoch 68: 9565 / 10000\n",
      "Epoch 69: 9566 / 10000\n",
      "Epoch 70: 9569 / 10000\n",
      "Epoch 71: 9570 / 10000\n",
      "Epoch 72: 9572 / 10000\n",
      "Epoch 73: 9573 / 10000\n",
      "Epoch 74: 9574 / 10000\n",
      "Epoch 75: 9573 / 10000\n",
      "Epoch 76: 9574 / 10000\n",
      "Epoch 77: 9574 / 10000\n",
      "Epoch 78: 9574 / 10000\n",
      "Epoch 79: 9574 / 10000\n",
      "Epoch 80: 9574 / 10000\n",
      "Epoch 81: 9575 / 10000\n",
      "Epoch 82: 9575 / 10000\n",
      "Epoch 83: 9574 / 10000\n",
      "Epoch 84: 9575 / 10000\n",
      "Epoch 85: 9576 / 10000\n",
      "Epoch 86: 9577 / 10000\n",
      "Epoch 87: 9576 / 10000\n",
      "Epoch 88: 9577 / 10000\n",
      "Epoch 89: 9578 / 10000\n",
      "Epoch 90: 9580 / 10000\n",
      "Epoch 91: 9580 / 10000\n",
      "Epoch 92: 9581 / 10000\n",
      "Epoch 93: 9580 / 10000\n",
      "Epoch 94: 9584 / 10000\n",
      "Epoch 95: 9586 / 10000\n",
      "Epoch 96: 9585 / 10000\n",
      "Epoch 97: 9586 / 10000\n",
      "Epoch 98: 9585 / 10000\n",
      "Epoch 99: 9587 / 10000\n",
      "Epoch 100: 9586 / 10000\n",
      "Epoch 101: 9587 / 10000\n",
      "Epoch 102: 9587 / 10000\n",
      "Epoch 103: 9585 / 10000\n",
      "Epoch 104: 9586 / 10000\n",
      "Epoch 105: 9587 / 10000\n",
      "Epoch 106: 9587 / 10000\n",
      "Epoch 107: 9588 / 10000\n",
      "Epoch 108: 9589 / 10000\n",
      "Epoch 109: 9590 / 10000\n",
      "Epoch 110: 9592 / 10000\n",
      "Epoch 111: 9592 / 10000\n",
      "Epoch 112: 9592 / 10000\n",
      "Epoch 113: 9592 / 10000\n",
      "Epoch 114: 9593 / 10000\n",
      "Epoch 115: 9593 / 10000\n",
      "Epoch 116: 9592 / 10000\n",
      "Epoch 117: 9592 / 10000\n",
      "Epoch 118: 9592 / 10000\n",
      "Epoch 119: 9592 / 10000\n",
      "Epoch 120: 9594 / 10000\n",
      "Epoch 121: 9595 / 10000\n",
      "Epoch 122: 9595 / 10000\n",
      "Epoch 123: 9595 / 10000\n",
      "Epoch 124: 9594 / 10000\n",
      "Epoch 125: 9594 / 10000\n",
      "Epoch 126: 9594 / 10000\n",
      "Epoch 127: 9597 / 10000\n",
      "Epoch 128: 9598 / 10000\n",
      "Epoch 129: 9597 / 10000\n",
      "Epoch 130: 9605 / 10000\n",
      "Epoch 131: 9600 / 10000\n",
      "Epoch 132: 9605 / 10000\n",
      "Epoch 133: 9605 / 10000\n",
      "Epoch 134: 9606 / 10000\n",
      "Epoch 135: 9606 / 10000\n",
      "Epoch 136: 9606 / 10000\n",
      "Epoch 137: 9607 / 10000\n",
      "Epoch 138: 9607 / 10000\n",
      "Epoch 139: 9606 / 10000\n",
      "Epoch 140: 9607 / 10000\n",
      "Epoch 141: 9607 / 10000\n",
      "Epoch 142: 9608 / 10000\n",
      "Epoch 143: 9608 / 10000\n",
      "Epoch 144: 9608 / 10000\n",
      "Epoch 145: 9610 / 10000\n",
      "Epoch 146: 9608 / 10000\n",
      "Epoch 147: 9609 / 10000\n",
      "Epoch 148: 9609 / 10000\n",
      "Epoch 149: 9608 / 10000\n",
      "Epoch 150: 9607 / 10000\n",
      "Epoch 151: 9605 / 10000\n",
      "Epoch 152: 9604 / 10000\n",
      "Epoch 153: 9604 / 10000\n",
      "Epoch 154: 9604 / 10000\n",
      "Epoch 155: 9606 / 10000\n",
      "Epoch 156: 9606 / 10000\n",
      "Epoch 157: 9606 / 10000\n",
      "Epoch 158: 9607 / 10000\n",
      "Epoch 159: 9606 / 10000\n",
      "Epoch 160: 9608 / 10000\n",
      "Epoch 161: 9608 / 10000\n",
      "Epoch 162: 9610 / 10000\n",
      "Epoch 163: 9609 / 10000\n",
      "Epoch 164: 9606 / 10000\n",
      "Epoch 165: 9607 / 10000\n",
      "Epoch 166: 9606 / 10000\n",
      "Epoch 167: 9606 / 10000\n",
      "Epoch 168: 9607 / 10000\n",
      "Epoch 169: 9607 / 10000\n",
      "Epoch 170: 9607 / 10000\n",
      "Epoch 171: 9609 / 10000\n",
      "Epoch 172: 9610 / 10000\n",
      "Epoch 173: 9610 / 10000\n",
      "Epoch 174: 9610 / 10000\n",
      "Epoch 175: 9609 / 10000\n",
      "Epoch 176: 9609 / 10000\n",
      "Epoch 177: 9609 / 10000\n",
      "Epoch 178: 9610 / 10000\n",
      "Epoch 179: 9609 / 10000\n",
      "Epoch 180: 9609 / 10000\n",
      "Epoch 181: 9609 / 10000\n",
      "Epoch 182: 9610 / 10000\n",
      "Epoch 183: 9610 / 10000\n",
      "Epoch 184: 9610 / 10000\n",
      "Epoch 185: 9612 / 10000\n",
      "Epoch 186: 9616 / 10000\n",
      "Epoch 187: 9611 / 10000\n",
      "Epoch 188: 9610 / 10000\n",
      "Epoch 189: 9609 / 10000\n",
      "Epoch 190: 9608 / 10000\n",
      "Epoch 191: 9608 / 10000\n",
      "Epoch 192: 9609 / 10000\n",
      "Epoch 193: 9610 / 10000\n",
      "Epoch 194: 9610 / 10000\n",
      "Epoch 195: 9611 / 10000\n",
      "Epoch 196: 9608 / 10000\n",
      "Epoch 197: 9608 / 10000\n",
      "Epoch 198: 9609 / 10000\n",
      "Epoch 199: 9605 / 10000\n",
      "Epoch 200: 9604 / 10000\n",
      "Epoch 201: 9609 / 10000\n",
      "Epoch 202: 9609 / 10000\n",
      "Epoch 203: 9609 / 10000\n",
      "Epoch 204: 9610 / 10000\n",
      "Epoch 205: 9610 / 10000\n",
      "Epoch 206: 9610 / 10000\n",
      "Epoch 207: 9611 / 10000\n",
      "Epoch 208: 9610 / 10000\n",
      "Epoch 209: 9610 / 10000\n",
      "Epoch 210: 9610 / 10000\n",
      "Epoch 211: 9610 / 10000\n",
      "Epoch 212: 9610 / 10000\n",
      "Epoch 213: 9610 / 10000\n",
      "Epoch 214: 9610 / 10000\n",
      "Epoch 215: 9610 / 10000\n",
      "Epoch 216: 9611 / 10000\n",
      "Epoch 217: 9611 / 10000\n",
      "Epoch 218: 9611 / 10000\n",
      "Epoch 219: 9611 / 10000\n",
      "Epoch 220: 9611 / 10000\n",
      "Epoch 221: 9611 / 10000\n",
      "Epoch 222: 9611 / 10000\n",
      "Epoch 223: 9612 / 10000\n",
      "Epoch 224: 9613 / 10000\n",
      "Epoch 225: 9612 / 10000\n",
      "Epoch 226: 9612 / 10000\n",
      "Epoch 227: 9612 / 10000\n",
      "Epoch 228: 9611 / 10000\n",
      "Epoch 229: 9610 / 10000\n",
      "Epoch 230: 9610 / 10000\n",
      "Epoch 231: 9611 / 10000\n",
      "Epoch 232: 9613 / 10000\n",
      "Epoch 233: 9613 / 10000\n",
      "Epoch 234: 9613 / 10000\n",
      "Epoch 235: 9613 / 10000\n",
      "Epoch 236: 9613 / 10000\n",
      "Epoch 237: 9613 / 10000\n",
      "Epoch 238: 9614 / 10000\n",
      "Epoch 239: 9614 / 10000\n",
      "Epoch 240: 9614 / 10000\n",
      "Epoch 241: 9614 / 10000\n",
      "Epoch 242: 9614 / 10000\n",
      "Epoch 243: 9614 / 10000\n",
      "Epoch 244: 9614 / 10000\n",
      "Epoch 245: 9614 / 10000\n",
      "Epoch 246: 9614 / 10000\n",
      "Epoch 247: 9614 / 10000\n",
      "Epoch 248: 9614 / 10000\n",
      "Epoch 249: 9615 / 10000\n",
      "Epoch 250: 9615 / 10000\n",
      "Epoch 251: 9614 / 10000\n",
      "Epoch 252: 9615 / 10000\n",
      "Epoch 253: 9614 / 10000\n",
      "Epoch 254: 9614 / 10000\n",
      "Epoch 255: 9614 / 10000\n",
      "Epoch 256: 9615 / 10000\n",
      "Epoch 257: 9613 / 10000\n",
      "Epoch 258: 9613 / 10000\n",
      "Epoch 259: 9613 / 10000\n",
      "Epoch 260: 9613 / 10000\n",
      "Epoch 261: 9613 / 10000\n",
      "Epoch 262: 9613 / 10000\n",
      "Epoch 263: 9613 / 10000\n",
      "Epoch 264: 9613 / 10000\n",
      "Epoch 265: 9613 / 10000\n",
      "Epoch 266: 9613 / 10000\n",
      "Epoch 267: 9613 / 10000\n",
      "Epoch 268: 9613 / 10000\n",
      "Epoch 269: 9614 / 10000\n",
      "Epoch 270: 9614 / 10000\n",
      "Epoch 271: 9614 / 10000\n",
      "Epoch 272: 9614 / 10000\n",
      "Epoch 273: 9614 / 10000\n",
      "Epoch 274: 9614 / 10000\n",
      "Epoch 275: 9614 / 10000\n",
      "Epoch 276: 9614 / 10000\n",
      "Epoch 277: 9614 / 10000\n",
      "Epoch 278: 9614 / 10000\n",
      "Epoch 279: 9614 / 10000\n",
      "Epoch 280: 9613 / 10000\n",
      "Epoch 281: 9612 / 10000\n",
      "Epoch 282: 9613 / 10000\n",
      "Epoch 283: 9614 / 10000\n",
      "Epoch 284: 9614 / 10000\n",
      "Epoch 285: 9613 / 10000\n",
      "Epoch 286: 9613 / 10000\n",
      "Epoch 287: 9613 / 10000\n",
      "Epoch 288: 9613 / 10000\n",
      "Epoch 289: 9613 / 10000\n",
      "Epoch 290: 9613 / 10000\n",
      "Epoch 291: 9613 / 10000\n",
      "Epoch 292: 9613 / 10000\n",
      "Epoch 293: 9614 / 10000\n",
      "Epoch 294: 9616 / 10000\n",
      "Epoch 295: 9614 / 10000\n",
      "Epoch 296: 9612 / 10000\n",
      "Epoch 297: 9612 / 10000\n",
      "Epoch 298: 9614 / 10000\n",
      "Epoch 299: 9616 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0056814062166457"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_3 = ComplexNetwork()\n",
    "CNN_3.load(\"test2_CNN_3\")\n",
    "rate = 5.48\n",
    "CNN_3.stochastic_gradient_descent(t_mat, t_labels, 300, 100, rate, 0.01/len(t_labels), \\\n",
    "                                    test_X = v_mat, test_y = v_labels, t=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Bootstrap: trial\"\"\"\n",
    "import csv\n",
    "NN1 = Network()\n",
    "NN1.load(\"FinalCell96Ver1\")\n",
    "prediction1 = np.argmax(NN1.forward(test_mat), axis=1)\n",
    "NN2 = ComplexNetwork()\n",
    "NN2.load(\"FinalCNNVer2\")\n",
    "prediction2 = np.argmax(NN2.forward(test_mat)[0], axis=1)\n",
    "NN3 = ComplexNetwork()\n",
    "NN3.load(\"test2_CNN_0\")\n",
    "prediction3 = np.argmax(NN3.forward(test_mat)[0], axis=1)\n",
    "NN4 = ComplexNetwork()\n",
    "NN4.load(\"test2_CNN_1\")\n",
    "prediction4 = np.argmax(NN4.forward(test_mat)[0], axis=1)\n",
    "NN5 = ComplexNetwork()\n",
    "NN5.load(\"Final9777Ver1\")\n",
    "prediction5 = np.argmax(NN5.forward(test_mat)[0], axis=1)\n",
    "NN6 = ComplexNetwork()\n",
    "NN6.load(\"test2_CNN_5\")\n",
    "prediction6 = np.argmax(NN6.forward(test_mat)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.vstack((prediction1, prediction2, prediction3, prediction4, prediction5, prediction6)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.04361031e-09   1.20288138e-07   7.02874034e-09 ...,   2.02279261e-04\n",
      "    7.73807790e-03   3.50079807e-02]\n",
      " [  6.41950118e-13   8.69858078e-01   3.08454351e-07 ...,   1.43845660e-04\n",
      "    3.89575697e-02   1.58292313e-06]\n",
      " [  1.57498381e-08   2.81276031e-07   2.85093413e-08 ...,   2.60393012e-04\n",
      "    3.36546734e-09   9.99999369e-01]\n",
      " ..., \n",
      " [  9.98903269e-01   1.68685978e-08   2.78388982e-06 ...,   2.52560391e-04\n",
      "    3.79162055e-05   1.59473576e-07]\n",
      " [  1.07690351e-09   1.02735978e-08   6.71610499e-06 ...,   3.31469554e-06\n",
      "    3.68319325e-05   1.02405847e-03]\n",
      " [  5.12553291e-09   2.25790319e-05   7.99097454e-07 ...,   2.54440879e-06\n",
      "    1.50230105e-01   5.05281522e-03]]\n",
      "[4 1 9 ..., 0 4 4]\n",
      "0.87\n",
      "[[ 975    0    1    0    0   12   14    0    6    2]\n",
      " [   0 1035    5    4    1    2    2    3   22    2]\n",
      " [   0    0  898    9    1    6   17    6   37    2]\n",
      " [   0    1    0  857    0   93    5    4   69   27]\n",
      " [   1    0    0    0  639    1   13    1    8  296]\n",
      " [   0    0    0    0    0  819   17    0   22   27]\n",
      " [   1    0    0    0    0    0  960    0   11    0]\n",
      " [   0    1    2    0    0    0    0  585    7  460]\n",
      " [   0    1    0    0    0    1    1    0  916   69]\n",
      " [   1    0    0    1    2    2    0    1    0 1016]]\n"
     ]
    }
   ],
   "source": [
    "NN4 = ComplexNetwork()\n",
    "NN4.load(\"test2_CNN_4\")\n",
    "yHat = NN4.forward(v_mat)[0]\n",
    "print(yHat)\n",
    "predicted = np.argmax(yHat, axis = 1)\n",
    "print(predicted)\n",
    "expected = np.argmax(v_labels, axis = 1)\n",
    "s = metrics.accuracy_score(expected, predicted)\n",
    "m = metrics.confusion_matrix(expected, predicted)\n",
    "print(s)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "prediction = []\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    prediction.append(mode(result[i])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('predicted_digits.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, lineterminator = '\\n')\n",
    "    for i, content in enumerate(prediction):\n",
    "        writer.writerow([i+1, content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
